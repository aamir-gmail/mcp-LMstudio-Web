Python Sandbox and Gradio UI for LM Studio
This project provides a complete ecosystem for executing Python code in a secure sandbox, tightly integrated with a Gradio-based web UI for interacting with Large Language Models from services like LM Studio.

Note: This setup has been tested and optimised for LM Studio v0.3.x using the qwen.qwen3-coder-30b-a3b-instruct model for robust tool-based code execution.

The full stack includes three main components:

Python Sandbox: A secure, containerised environment for running Python code with a rich data science and machine learning stack.

Artifact Server: A lightweight web server to store and serve files (plots, images, data) generated by the sandbox.

Gradio UI: A user-friendly web interface to chat with an LLM, which can delegate code execution tasks to the Python Sandbox and display the results, including images and files.

Features
Full-Stack Application: A complete, multi-container solution with a web UI, code execution backend, and file server.

Interactive UI: A Gradio interface for chat, completions, and direct sandbox interaction.

Isolated Code Execution: Each script runs in a separate subprocess within its own temporary directory to ensure security and prevent interference.

Rich Python Environment: Built on Conda with Python 3.11, including libraries like NumPy, Pandas, Scikit-learn, Matplotlib, and more.

Automatic File & Image Capture: Automatically detects images, PDFs, and other files generated by code and makes them accessible through the UI and direct URLs.

Automatic Matplotlib Saving: Saves any open Matplotlib figures at the end of a script's execution automatically.

Containerized & Composable: Fully containerized with Docker and orchestrated with Docker Compose for easy, reproducible deployment.

Highly Configurable: Key parameters for all services (ports, URLs, models) are configurable through environment variables.

System Architecture
The complete system consists of three interconnected services orchestrated by Docker Compose, which communicate with your local LM Studio instance.

Gradio UI (ui): This is the user-facing web application. It communicates with an LLM service (like LM Studio) to get responses and can trigger tool calls to the Sandbox/MCP service when the model wants to execute code.

Sandbox/MCP (mcp): The core code execution engine. It exposes a REST API that the Gradio UI calls to run Python code. It writes all output files to a shared volume.

Artifacts Server (artifacts): A simple Python HTTP server that serves the files written by the Sandbox container, making them accessible to the user's browser.

All three containers share a Docker volume (sandbox_artifacts) to persist and share the generated files.

Prerequisites
Docker

Docker Compose

Full Stack Deployment with Gradio UI
This is the recommended method for running the complete application.

Step 1: Build the Docker Images

First, build the Docker image for the sandbox. The UI image will be built automatically by Docker Compose.

Sandbox Image:

docker build -t python-sandbox -f Dockerfile .

Step 2: Configure Environment Variables

Create a file named .env in the same directory as the docker-compose.external_v3.yml file. This file is the central point for configuring all services.

Important: You must replace <YOUR_HOST_IP> with the actual local network IP address of the machine running Docker (e.g., 192.168.1.100). Do not use localhost or 127.0.0.1, as the containers need to communicate with each other and your browser needs to access the services.

Your .env file should look like this:

# --- Core Network Configuration ---
# !!! IMPORTANT: Replace with your machine's actual LAN IP address !!!
HOST_BASE_IP=<YOUR_HOST_IP>

# --- Port Configuration ---
# Port for the Gradio Web UI
UI_PORT=7860
# Port for the Sandbox/MCP REST API
MCP_PORT=8000
# Port for the Artifacts file server
ARTIFACTS_PORT=18080
# Port for your LM Studio API server
LMSTUDIO_API_PORT=1234

Environment Variable Reference
Variable

Description

Service(s) Used By

HOST_BASE_IP

(Required) The LAN IP address of the host machine. Used to construct URLs for inter-service communication.

UI, Sandbox

UI_PORT

The external port for accessing the Gradio Web UI.

UI

MCP_PORT

The port where the Python Sandbox's REST API will be accessible.

UI, Sandbox

ARTIFACTS_PORT

The port for the simple HTTP server that serves generated files (images, plots, etc.).

UI, Sandbox, Artifacts

LMSTUDIO_API_PORT

The port where your LM Studio's local inference server is running.

UI

Step 3: Launch the Application Stack

From the directory containing your docker-compose.external_v3.yml and .env files, run:

docker-compose -f docker-compose.external_v3.yml up --build -d

This command will:

Build the UI image if it doesn't exist.

Pull the required Python image for the artifact server.

Create and start all three containers (ui, mcp, artifacts) in the correct order.

-d runs the containers in detached mode.

Step 4: Access the UI

Once all containers are running, you can access the Gradio web interface in your browser at http://<YOUR_HOST_IP>:7860.

Integration with LM Studio (MCP File)
This method allows you to use the Python Sandbox directly from LM Studio's built-in chat UI, without needing the Gradio interface.

Step 1: Create mcp.json

Create a file named mcp.json inside your LM Studio configuration folder (e.g., ~/.cache/lm-studio/mcp.json on Linux/Mac, or %AppData%\LM Studio\mcp.json on Windows).

Step 2: Add Server Configuration

Paste the following content into the mcp.json file. You must replace the placeholder values.

{
  "mcpServers": {
    "python-sandbox": {
      "command": "docker",
      "args": [
        "run",
        "--rm",
        "--init",
        "-i",
        "-v",
        "/path/to/your/artifacts:/app/temp",
        "python-sandbox:latest",
        "python",
        "/app/mcp_server.py",
        "--stdio"
      ],
      "env": {
        "PUBLIC_BASE_URL": "http://<YOUR_HOST_IP>:<ARTIFACTS_PORT>",
        "PUBLIC_BASE_URL_MODE": "plain"
      }
    }
  }
}

Configuration Details:

/path/to/your/artifacts: Replace this with the absolute path to a directory on your host machine where generated files should be stored. This directory will be mounted into the container.

<YOUR_HOST_IP>: Replace with the same host IP address you used in the .env file.

<ARTIFACTS_PORT>: Replace with the port for the artifact server (e.g., 18080 from the .env file).

Step 3: Ensure Artifact Server is Running

This setup requires an artifact server to be running to serve the generated files. The easiest way to do this is to run the artifacts service from the full stack deployment:

# Make sure your .env file is configured correctly
docker-compose -f docker-compose.external_v3.yml up -d artifacts

Step 4: Restart LM Studio

Close and reopen LM Studio for it to load the new mcp.json configuration. You should now see "python-sandbox" as an available tool in the chat interface.

Standalone Sandbox Deployment (Without UI)
If you only need the Python code execution sandbox and its REST API, you can run it as a standalone container.

Mode 1: Run as an MCP Tool (Simple)
This mode is ideal for integration with tools that communicate over stdio.

First, create a volume: docker volume create sandbox_artifacts

Then, run the container:

docker run -i --rm \
  -v sandbox_artifacts:/app/temp \
  -p 8000:8000 \
  -e PUBLIC_BASE_URL="http://localhost:8000" \
  --name python-sandbox-mcp \
  python-sandbox

Mode 2: Run as a Standalone REST API
This mode is useful for interacting with the sandbox directly over HTTP.

docker run -d --rm \
  -v sandbox_artifacts:/app/temp \
  -p 8000:8000 \
  --name python-sandbox-rest \
  python-sandbox python /app/server_rest.py

The API documentation will be available at http://localhost:8000/docs.

Project File Structure
.
├── .env                        # Main configuration file for ports and IPs
├── Dockerfile                  # For the Python Sandbox
├── docker-compose.yml          # Original simple compose file
├── mcp_server.py               # Sandbox MCP (stdio) entry point
├── rest_app.py                 # FastAPI app for the sandbox
├── sandbox_core.py             # Core code execution logic
└── server_rest.py              # Standalone REST API entry point

# --- UI Components ---
├── ui.Dockerfile               # For the Gradio UI
├── docker-compose.external_v3.yml # Full stack deployment file
├── app_gradio_..._v9.py        # Gradio UI application logic
├── requirements.ui.txt         # Python dependencies for the UI
├── sys_prompt.py               # System prompt for the LLM
└── ui_logging.py               # Logging configuration for the UI
